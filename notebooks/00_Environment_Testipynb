{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843a2526-edf2-48dd-a1d6-a1e7be908a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Python version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
      "\n",
      "Testing library imports...\n",
      "\n",
      "‚úÖ pandas          - Data manipulation\n",
      "‚úÖ numpy           - Numerical computing\n",
      "‚úÖ spacy           - NLP and entity recognition\n",
      "‚úÖ pdfplumber      - PDF text extraction\n",
      "‚úÖ docx            - Word document reading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers    - Advanced NLP models\n",
      "‚úÖ sklearn         - Machine learning\n",
      "‚úÖ matplotlib      - Data visualization\n",
      "\n",
      "==================================================\n",
      "Testing spaCy language model...\n",
      "==================================================\n",
      "‚úÖ spaCy model loaded successfully\n",
      "   Test sentence: 'John Doe works at Google in 2024'\n",
      "   Entities found: [('John Doe', 'PERSON'), ('Google', 'ORG'), ('2024', 'DATE')]\n",
      "\n",
      "==================================================\n",
      "üéâ RESULTS: 8/8 libraries working\n",
      "==================================================\n",
      "\n",
      "‚úÖ All libraries working perfectly!\n",
      "üöÄ Ready to start building your Resume Parser!\n"
     ]
    }
   ],
   "source": [
    "# Test all libraries\n",
    "import sys\n",
    "print(f\"‚úÖ Python version: {sys.version}\\n\")\n",
    "\n",
    "# Test each library\n",
    "libraries = {\n",
    "    'pandas': 'Data manipulation',\n",
    "    'numpy': 'Numerical computing',\n",
    "    'spacy': 'NLP and entity recognition',\n",
    "    'pdfplumber': 'PDF text extraction',\n",
    "    'docx': 'Word document reading',\n",
    "    'transformers': 'Advanced NLP models',\n",
    "    'sklearn': 'Machine learning',\n",
    "    'matplotlib': 'Data visualization'\n",
    "}\n",
    "\n",
    "print(\"Testing library imports...\\n\")\n",
    "success_count = 0\n",
    "failed_libs = []\n",
    "\n",
    "for lib, purpose in libraries.items():\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úÖ {lib:15} - {purpose}\")\n",
    "        success_count += 1\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib:15} - FAILED\")\n",
    "        failed_libs.append(lib)\n",
    "\n",
    "# Test spaCy model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing spaCy language model...\")\n",
    "print(\"=\"*50)\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(\"John Doe works at Google in 2024\")\n",
    "    print(\"‚úÖ spaCy model loaded successfully\")\n",
    "    print(f\"   Test sentence: 'John Doe works at Google in 2024'\")\n",
    "    print(f\"   Entities found: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå spaCy model failed: {e}\")\n",
    "    failed_libs.append('spacy_model')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üéâ RESULTS: {success_count}/{len(libraries)} libraries working\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if failed_libs:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed libraries: {', '.join(failed_libs)}\")\n",
    "    print(\"We'll need to reinstall these.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All libraries working perfectly!\")\n",
    "    print(\"üöÄ Ready to start building your Resume Parser!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043f7588-03a6-4805-8b05-ca4f465e098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created: resume_priya_sharma.txt\n",
      "‚úÖ Created: resume_rahul_verma.txt\n",
      "‚úÖ Created: resume_anjali_patel.txt\n",
      "\n",
      "üéâ Successfully created 3 sample resumes in ../data/resumes/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create sample resumes\n",
    "resumes_folder = \"../data/resumes/\"\n",
    "\n",
    "# Sample Resume 1 - Software Engineer\n",
    "resume1 = \"\"\"\n",
    "PRIYA SHARMA\n",
    "Email: priya.sharma@email.com\n",
    "Phone: +91-9876543210\n",
    "Location: Bangalore, India\n",
    "\n",
    "PROFESSIONAL SUMMARY\n",
    "Software Engineer with 3 years of experience in Python, Machine Learning, and Data Analysis.\n",
    "\n",
    "SKILLS\n",
    "- Programming: Python, Java, SQL\n",
    "- Machine Learning: Scikit-learn, TensorFlow, Pandas\n",
    "- Tools: Git, Docker, Jupyter\n",
    "- Soft Skills: Team Leadership, Problem Solving\n",
    "\n",
    "WORK EXPERIENCE\n",
    "\n",
    "Data Scientist | TechCorp India | June 2021 - Present\n",
    "- Built machine learning models for customer segmentation\n",
    "- Improved prediction accuracy by 25%\n",
    "- Worked with cross-functional teams of 10+ members\n",
    "\n",
    "Software Engineer Intern | StartupXYZ | Jan 2020 - May 2021\n",
    "- Developed REST APIs using Python Flask\n",
    "- Automated data pipelines reducing processing time by 40%\n",
    "\n",
    "EDUCATION\n",
    "Bachelor of Technology in Computer Science\n",
    "Indian Institute of Technology (IIT) Delhi\n",
    "Graduated: 2020\n",
    "GPA: 8.5/10\n",
    "\n",
    "CERTIFICATIONS\n",
    "- AWS Certified Developer Associate (2022)\n",
    "- Google Data Analytics Professional Certificate (2021)\n",
    "\n",
    "PROJECTS\n",
    "- Built a recommendation system using collaborative filtering\n",
    "- Created a sentiment analysis tool for social media data\n",
    "\"\"\"\n",
    "\n",
    "# Sample Resume 2 - Marketing Manager\n",
    "resume2 = \"\"\"\n",
    "RAHUL VERMA\n",
    "rahul.verma@gmail.com | +91-9123456789 | Mumbai, Maharashtra\n",
    "\n",
    "OBJECTIVE\n",
    "Experienced Marketing Manager seeking to leverage 5+ years of digital marketing expertise.\n",
    "\n",
    "CORE COMPETENCIES\n",
    "Digital Marketing, SEO/SEM, Content Strategy, Social Media Management, Google Analytics\n",
    "Team Management, Budget Planning, Campaign Optimization\n",
    "\n",
    "PROFESSIONAL EXPERIENCE\n",
    "\n",
    "Senior Marketing Manager | BrandCo Ltd. | March 2020 - Present\n",
    "- Led digital marketing campaigns with $500K+ annual budget\n",
    "- Increased website traffic by 150% through SEO optimization\n",
    "- Managed team of 8 marketing professionals\n",
    "- Launched 20+ successful product campaigns\n",
    "\n",
    "Marketing Executive | MediaHub Agency | July 2018 - Feb 2020\n",
    "- Coordinated social media strategy across 5 platforms\n",
    "- Improved engagement rates by 80%\n",
    "- Created content calendars and managed influencer partnerships\n",
    "\n",
    "EDUCATION\n",
    "MBA in Marketing\n",
    "XLRI Jamshedpur\n",
    "Completed: 2018\n",
    "\n",
    "Bachelor of Commerce\n",
    "Mumbai University\n",
    "Completed: 2016\n",
    "\n",
    "ACHIEVEMENTS\n",
    "- Winner of Best Digital Campaign Award 2022\n",
    "- Increased ROI by 200% for key client accounts\n",
    "- Published articles in Marketing Today magazine\n",
    "\"\"\"\n",
    "\n",
    "# Sample Resume 3 - Fresh Graduate\n",
    "resume3 = \"\"\"\n",
    "ANJALI PATEL\n",
    "Email: anjali.patel2024@email.com\n",
    "Phone: +91-9988776655\n",
    "LinkedIn: linkedin.com/in/anjalipatel\n",
    "\n",
    "EDUCATION\n",
    "Master of Science in Data Science\n",
    "Pune University\n",
    "Expected Graduation: May 2024\n",
    "CGPA: 9.2/10\n",
    "\n",
    "Bachelor of Engineering in Information Technology  \n",
    "Gujarat Technological University\n",
    "Graduated: 2022\n",
    "CGPA: 8.8/10\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "- Languages: Python, R, SQL, JavaScript\n",
    "- Libraries: Pandas, NumPy, Matplotlib, Scikit-learn, spaCy\n",
    "- Tools: Jupyter, Git, Tableau, Power BI\n",
    "- Databases: MySQL, MongoDB\n",
    "\n",
    "ACADEMIC PROJECTS\n",
    "\n",
    "Resume Parser using NLP | Jan 2024 - Present\n",
    "- Building an automated resume parsing system using spaCy and transformers\n",
    "- Extracting entities like skills, education, experience from resumes\n",
    "- Achieved 85% accuracy in entity recognition\n",
    "\n",
    "Customer Churn Prediction | Sep 2023 - Dec 2023\n",
    "- Predicted customer churn using Random Forest and XGBoost\n",
    "- Analyzed dataset of 50,000+ customer records\n",
    "- Improved model performance to 92% accuracy\n",
    "\n",
    "INTERNSHIPS\n",
    "\n",
    "Data Analytics Intern | FinTech Solutions Pvt. Ltd. | Summer 2023\n",
    "- Analyzed transaction data to identify fraud patterns\n",
    "- Created dashboards using Tableau for executive reporting\n",
    "- Reduced false positive alerts by 30%\n",
    "\n",
    "CERTIFICATIONS\n",
    "- Python for Data Science (Coursera, 2023)\n",
    "- Machine Learning Specialization (DeepLearning.AI, 2023)\n",
    "\n",
    "EXTRACURRICULAR\n",
    "- Vice President, Data Science Club, Pune University\n",
    "- Participated in 3 Kaggle competitions, highest rank: Top 15%\n",
    "\"\"\"\n",
    "\n",
    "# Save the resumes\n",
    "resumes = {\n",
    "    \"resume_priya_sharma.txt\": resume1,\n",
    "    \"resume_rahul_verma.txt\": resume2,\n",
    "    \"resume_anjali_patel.txt\": resume3\n",
    "}\n",
    "\n",
    "for filename, content in resumes.items():\n",
    "    filepath = os.path.join(resumes_folder, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"‚úÖ Created: {filename}\")\n",
    "\n",
    "print(f\"\\nüéâ Successfully created {len(resumes)} sample resumes in {resumes_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c193e74b-2002-45bb-a54f-0327315d5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created: job_data_scientist.txt\n",
      "‚úÖ Created: job_marketing_manager.txt\n",
      "\n",
      "üéâ Successfully created 2 sample job descriptions in ../data/job_descriptions/\n"
     ]
    }
   ],
   "source": [
    "# Create sample job descriptions\n",
    "jobs_folder = \"../data/job_descriptions/\"\n",
    "\n",
    "# Job Description 1 - Data Scientist\n",
    "job1 = \"\"\"\n",
    "JOB TITLE: Data Scientist\n",
    "COMPANY: TechVentures Pvt. Ltd.\n",
    "LOCATION: Bangalore, India\n",
    "JOB TYPE: Full-time\n",
    "\n",
    "ABOUT THE ROLE:\n",
    "We are seeking a talented Data Scientist to join our AI team. You will work on cutting-edge \n",
    "machine learning projects and help drive data-driven decision making across the organization.\n",
    "\n",
    "REQUIRED QUALIFICATIONS:\n",
    "- Bachelor's or Master's degree in Computer Science, Statistics, or related field\n",
    "- 2-4 years of experience in data science or machine learning roles\n",
    "- Strong programming skills in Python\n",
    "- Experience with machine learning libraries: Scikit-learn, TensorFlow, or PyTorch\n",
    "- Proficiency in SQL and database management\n",
    "- Experience with data visualization tools (Tableau, Power BI, or matplotlib)\n",
    "\n",
    "PREFERRED SKILLS:\n",
    "- Experience with NLP and text analytics\n",
    "- Knowledge of big data technologies (Spark, Hadoop)\n",
    "- Strong statistical analysis skills\n",
    "- Experience with cloud platforms (AWS, Azure, or GCP)\n",
    "- Published research or contributions to open-source projects\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "- Build and deploy machine learning models for various business use cases\n",
    "- Analyze large datasets to extract actionable insights\n",
    "- Collaborate with cross-functional teams including engineers and product managers\n",
    "- Present findings to stakeholders and leadership team\n",
    "- Mentor junior data scientists\n",
    "\n",
    "WHAT WE OFFER:\n",
    "- Competitive salary: ‚Çπ15-25 LPA\n",
    "- Health insurance and wellness benefits\n",
    "- Flexible work hours and remote work options\n",
    "- Learning and development budget\n",
    "- Opportunity to work on innovative AI projects\n",
    "\"\"\"\n",
    "\n",
    "# Job Description 2 - Digital Marketing Manager\n",
    "job2 = \"\"\"\n",
    "POSITION: Digital Marketing Manager\n",
    "ORGANIZATION: GrowthMax Marketing Agency\n",
    "LOCATION: Mumbai, Maharashtra (Hybrid)\n",
    "EMPLOYMENT TYPE: Full-time\n",
    "\n",
    "JOB SUMMARY:\n",
    "GrowthMax is looking for an experienced Digital Marketing Manager to lead our client campaigns \n",
    "and drive measurable results. The ideal candidate has a proven track record in digital marketing \n",
    "and team leadership.\n",
    "\n",
    "MINIMUM REQUIREMENTS:\n",
    "- Bachelor's degree in Marketing, Business, or related field; MBA preferred\n",
    "- 5+ years of experience in digital marketing\n",
    "- Proven experience managing marketing budgets of $100K+\n",
    "- Strong understanding of SEO, SEM, and social media marketing\n",
    "- Experience with Google Analytics, Google Ads, and Facebook Ads Manager\n",
    "- Excellent written and verbal communication skills\n",
    "\n",
    "NICE TO HAVE:\n",
    "- Google Ads Certification\n",
    "- HubSpot or similar marketing automation platform experience\n",
    "- Experience in B2B and B2C marketing\n",
    "- Knowledge of content management systems (WordPress, etc.)\n",
    "- Video marketing and production experience\n",
    "\n",
    "KEY RESPONSIBILITIES:\n",
    "- Develop and execute comprehensive digital marketing strategies\n",
    "- Manage and mentor a team of 5-10 marketing professionals\n",
    "- Oversee SEO/SEM, email marketing, social media, and content marketing campaigns\n",
    "- Track and analyze campaign performance using analytics tools\n",
    "- Manage client relationships and present campaign results\n",
    "- Stay updated with latest marketing trends and technologies\n",
    "- Allocate and optimize marketing budgets\n",
    "\n",
    "BENEFITS:\n",
    "- Annual salary: ‚Çπ12-18 LPA\n",
    "- Performance-based bonuses\n",
    "- Professional development opportunities\n",
    "- Work-life balance initiatives\n",
    "- Modern office with collaborative workspace\n",
    "\"\"\"\n",
    "\n",
    "# Save job descriptions\n",
    "jobs = {\n",
    "    \"job_data_scientist.txt\": job1,\n",
    "    \"job_marketing_manager.txt\": job2\n",
    "}\n",
    "\n",
    "for filename, content in jobs.items():\n",
    "    filepath = os.path.join(jobs_folder, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"‚úÖ Created: {filename}\")\n",
    "\n",
    "print(f\"\\nüéâ Successfully created {len(jobs)} sample job descriptions in {jobs_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24ee0c0-497b-4e04-a69e-fa047474619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ RESUMES:\n",
      "--------------------------------------------------\n",
      "  ‚úì resume_anjali_patel.txt\n",
      "  ‚úì resume_priya_sharma.txt\n",
      "  ‚úì resume_rahul_verma.txt\n",
      "\n",
      "üìÅ JOB DESCRIPTIONS:\n",
      "--------------------------------------------------\n",
      "  ‚úì job_data_scientist.txt\n",
      "  ‚úì job_marketing_manager.txt\n",
      "\n",
      "‚úÖ All sample files created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files\n",
    "print(\"üìÅ RESUMES:\")\n",
    "print(\"-\" * 50)\n",
    "for file in os.listdir(\"../data/resumes/\"):\n",
    "    print(f\"  ‚úì {file}\")\n",
    "\n",
    "print(\"\\nüìÅ JOB DESCRIPTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "for file in os.listdir(\"../data/job_descriptions/\"):\n",
    "    print(f\"  ‚úì {file}\")\n",
    "\n",
    "print(\"\\n‚úÖ All sample files created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da7906e-85cb-4456-93a9-478bb510568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ RESUME CONTENT:\n",
      "======================================================================\n",
      "\n",
      "PRIYA SHARMA\n",
      "Email: priya.sharma@email.com\n",
      "Phone: +91-9876543210\n",
      "Location: Bangalore, India\n",
      "\n",
      "PROFESSIONAL SUMMARY\n",
      "Software Engineer with 3 years of experience in Python, Machine Learning, and Data Analysis.\n",
      "\n",
      "SKILLS\n",
      "- Programming: Python, Java, SQL\n",
      "- Machine Learning: Scikit-learn, TensorFlow, Pandas\n",
      "- Tools: Git, Docker, Jupyter\n",
      "- Soft Skills: Team Leadership, Problem Solving\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Data Scientist | TechCorp India | June 2021 - Present\n",
      "- Built machine learning models for customer segmentation\n",
      "- Improved prediction accuracy by 25%\n",
      "- Worked with cross-functional teams of 10+ members\n",
      "\n",
      "Software Engineer Intern | StartupXYZ | Jan 2020 - May 2021\n",
      "- Developed REST APIs using Python Flask\n",
      "- Automated data pipelines reducing processing time by 40%\n",
      "\n",
      "EDUCATION\n",
      "Bachelor of Technology in Computer Science\n",
      "Indian Institute of Technology (IIT) Delhi\n",
      "Graduated: 2020\n",
      "GPA: 8.5/10\n",
      "\n",
      "CERTIFICATIONS\n",
      "- AWS Certified Developer Associate (2022)\n",
      "- Google Data Analytics Professional Certificate (2021)\n",
      "\n",
      "PROJECTS\n",
      "- Built a recommendation system using collaborative filtering\n",
      "- Created a sentiment analysis tool for social media data\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Successfully loaded resume!\n",
      "üìä Resume length: 1131 characters\n",
      "üìä Number of lines: 40\n"
     ]
    }
   ],
   "source": [
    "# Read a single resume file\n",
    "resume_path = \"../data/resumes/resume_priya_sharma.txt\"\n",
    "\n",
    "# Open and read the file\n",
    "with open(resume_path, 'r', encoding='utf-8') as file:\n",
    "    resume_text = file.read()\n",
    "\n",
    "# Display the content\n",
    "print(\"üìÑ RESUME CONTENT:\")\n",
    "print(\"=\" * 70)\n",
    "print(resume_text)\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úÖ Successfully loaded resume!\")\n",
    "print(f\"üìä Resume length: {len(resume_text)} characters\")\n",
    "print(f\"üìä Number of lines: {len(resume_text.split(chr(10)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf98243-3b41-4d3c-a020-851fb0dfb5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö LOADED RESUMES:\n",
      "======================================================================\n",
      "\n",
      "‚úÖ resume_anjali_patel.txt\n",
      "   Characters: 1453\n",
      "   Lines: 50\n",
      "   First 100 characters: \n",
      "ANJALI PATEL\n",
      "Email: anjali.patel2024@email.com\n",
      "Phone: +91-9988776655\n",
      "LinkedIn: linkedin.com/in/anja...\n",
      "\n",
      "‚úÖ resume_priya_sharma.txt\n",
      "   Characters: 1131\n",
      "   Lines: 40\n",
      "   First 100 characters: \n",
      "PRIYA SHARMA\n",
      "Email: priya.sharma@email.com\n",
      "Phone: +91-9876543210\n",
      "Location: Bangalore, India\n",
      "\n",
      "PROFES...\n",
      "\n",
      "‚úÖ resume_rahul_verma.txt\n",
      "   Characters: 1125\n",
      "   Lines: 38\n",
      "   First 100 characters: \n",
      "RAHUL VERMA\n",
      "rahul.verma@gmail.com | +91-9123456789 | Mumbai, Maharashtra\n",
      "\n",
      "OBJECTIVE\n",
      "Experienced Mar...\n",
      "\n",
      "======================================================================\n",
      "üéâ Total resumes loaded: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to read all resumes from a folder\n",
    "def load_all_resumes(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .txt files from the resumes folder\n",
    "    Returns a dictionary: {filename: resume_text}\n",
    "    \"\"\"\n",
    "    resumes = {}\n",
    "    \n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Loop through each file\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):  # Only read .txt files\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the file\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                resumes[filename] = file.read()\n",
    "    \n",
    "    return resumes\n",
    "\n",
    "# Load all resumes\n",
    "resumes_folder = \"../data/resumes/\"\n",
    "all_resumes = load_all_resumes(resumes_folder)\n",
    "\n",
    "# Display summary\n",
    "print(\"üìö LOADED RESUMES:\")\n",
    "print(\"=\" * 70)\n",
    "for filename, content in all_resumes.items():\n",
    "    print(f\"\\n‚úÖ {filename}\")\n",
    "    print(f\"   Characters: {len(content)}\")\n",
    "    print(f\"   Lines: {len(content.split(chr(10)))}\")\n",
    "    print(f\"   First 100 characters: {content[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéâ Total resumes loaded: {len(all_resumes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9ce1d1-0e02-41d1-b56e-4793d1fe3658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº LOADED JOB DESCRIPTIONS:\n",
      "======================================================================\n",
      "\n",
      "‚úÖ job_data_scientist.txt\n",
      "   Characters: 1534\n",
      "   Lines: 39\n",
      "   Job Title: \n",
      "\n",
      "‚úÖ job_marketing_manager.txt\n",
      "   Characters: 1648\n",
      "   Lines: 42\n",
      "   Job Title: \n",
      "\n",
      "======================================================================\n",
      "üéâ Total job descriptions loaded: 2\n"
     ]
    }
   ],
   "source": [
    "# Load all job descriptions\n",
    "jobs_folder = \"../data/job_descriptions/\"\n",
    "all_jobs = load_all_resumes(jobs_folder)  # We can reuse the same function!\n",
    "\n",
    "# Display summary\n",
    "print(\"üíº LOADED JOB DESCRIPTIONS:\")\n",
    "print(\"=\" * 70)\n",
    "for filename, content in all_jobs.items():\n",
    "    print(f\"\\n‚úÖ {filename}\")\n",
    "    print(f\"   Characters: {len(content)}\")\n",
    "    print(f\"   Lines: {len(content.split(chr(10)))}\")\n",
    "    \n",
    "    # Extract job title (first line usually)\n",
    "    first_line = content.split('\\n')[0]\n",
    "    print(f\"   Job Title: {first_line}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéâ Total job descriptions loaded: {len(all_jobs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a104f17-0cd1-4687-b946-5b19f29dc4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RESUME SUMMARY TABLE:\n",
      "======================================================================\n",
      "               Filename Candidate Name  Character Count  Line Count   Status\n",
      "resume_anjali_patel.txt                            1453          50 ‚úÖ Loaded\n",
      "resume_priya_sharma.txt                            1131          40 ‚úÖ Loaded\n",
      " resume_rahul_verma.txt                            1125          38 ‚úÖ Loaded\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a summary dataframe for resumes\n",
    "resume_summary = []\n",
    "\n",
    "for filename, content in all_resumes.items():\n",
    "    # Extract candidate name (usually first line)\n",
    "    lines = content.split('\\n')\n",
    "    name = lines[0].strip() if lines else \"Unknown\"\n",
    "    \n",
    "    resume_summary.append({\n",
    "        'Filename': filename,\n",
    "        'Candidate Name': name,\n",
    "        'Character Count': len(content),\n",
    "        'Line Count': len(lines),\n",
    "        'Status': '‚úÖ Loaded'\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_resumes = pd.DataFrame(resume_summary)\n",
    "\n",
    "print(\"üìä RESUME SUMMARY TABLE:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_resumes.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c006fa12-1a65-4c6b-b31f-5516f12384de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ ORIGINAL TEXT (first 200 chars):\n",
      "======================================================================\n",
      "\n",
      "PRIYA SHARMA\n",
      "Email: priya.sharma@email.com\n",
      "Phone: +91-9876543210\n",
      "Location: Bangalore, India\n",
      "\n",
      "PROFESSIONAL SUMMARY\n",
      "Software Engineer with 3 years of experience in Python, Machine Learning, and Data An\n",
      "\n",
      "\n",
      "üßπ PREPROCESSED TEXT (first 200 chars):\n",
      "======================================================================\n",
      "priya sharma email: priya.sharma@email.com phone: +91-9876543210 location: bangalore, india professional summary software engineer with 3 years of experience in python, machine learning, and data anal\n",
      "\n",
      "\n",
      "üî§ TOKENIZED (first 30 tokens):\n",
      "======================================================================\n",
      "['priya', 'sharma', 'email', 'priya.sharma@email.com', 'phone', '+91', '9876543210', 'location', 'bangalore', 'india', 'professional', 'summary', 'software', 'engineer', '3', 'years', 'experience', 'python', 'machine', 'learning', 'data', 'analysis', 'skills', 'programming', 'python', 'java', 'sql', 'machine', 'learning', 'scikit']\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "üìä Total tokens extracted: 131\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess resume text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep important ones like @ . -\n",
    "    text = re.sub(r'[^\\w\\s@.\\-+(),:;]', '', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using spaCy\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "# Test preprocessing on one resume\n",
    "sample_resume = all_resumes['resume_priya_sharma.txt']\n",
    "\n",
    "print(\"üìÑ ORIGINAL TEXT (first 200 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample_resume[:200])\n",
    "\n",
    "print(\"\\n\\nüßπ PREPROCESSED TEXT (first 200 chars):\")\n",
    "print(\"=\" * 70)\n",
    "preprocessed = preprocess_text(sample_resume)\n",
    "print(preprocessed[:200])\n",
    "\n",
    "print(\"\\n\\nüî§ TOKENIZED (first 30 tokens):\")\n",
    "print(\"=\" * 70)\n",
    "tokens = tokenize_text(preprocessed)\n",
    "print(tokens[:30])\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"üìä Total tokens extracted: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e83d973-a1c5-43d6-b279-791156110766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç EXTRACTING ENTITIES FROM PRIYA SHARMA'S RESUME:\n",
      "======================================================================\n",
      "\n",
      "NAMES:\n",
      "  ‚úì Machine Learning\n",
      "  ‚úì Pandas\n",
      "  ‚úì Java\n",
      "  ‚úì Jupyter\n",
      "- Soft\n",
      "  ‚úì Docker\n",
      "\n",
      "ORGANIZATIONS:\n",
      "  ‚úì Data Scientist\n",
      "  ‚úì CERTIFICATIONS\n",
      "  ‚úì Google Data Analytics Professional Certificate\n",
      "  ‚úì Bachelor of Technology\n",
      "  ‚úì TensorFlow\n",
      "  ‚úì IIT\n",
      "  ‚úì GPA\n",
      "  ‚úì Python\n",
      "  ‚úì Data Analysis\n",
      "\n",
      "DATES:\n",
      "  ‚úì June 2021 - Present\n",
      "  ‚úì Jan 2020 -\n",
      "  ‚úì 10+\n",
      "  ‚úì 2021\n",
      "  ‚úì 3 years\n",
      "\n",
      "LOCATIONS:\n",
      "  ‚úì SHARMA\n",
      "  ‚úì Flask\n",
      "  ‚úì India\n",
      "  ‚úì Delhi\n",
      "\n",
      "EMAILS:\n",
      "  ‚úì priya.sharma@email.com\n",
      "\n",
      "PHONES:\n",
      "  ‚úì +91-9876543210\n",
      "\n",
      "SKILLS:\n",
      "  ‚úì scikit-learn\n",
      "  ‚úì docker\n",
      "  ‚úì java\n",
      "  ‚úì aws\n",
      "  ‚úì machine learning\n",
      "  ‚úì python\n",
      "  ‚úì tensorflow\n",
      "  ‚úì sql\n",
      "  ‚úì git\n",
      "  ‚úì pandas\n",
      "  ‚úì flask\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Entity extraction complete!\n"
     ]
    }
   ],
   "source": [
    "def extract_entities_from_resume(text):\n",
    "    \"\"\"\n",
    "    Extract key information from resume using spaCy NER\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entities = {\n",
    "        'names': [],\n",
    "        'organizations': [],\n",
    "        'dates': [],\n",
    "        'locations': [],\n",
    "        'emails': [],\n",
    "        'phones': [],\n",
    "        'skills': []\n",
    "    }\n",
    "    \n",
    "    # Extract named entities using spaCy\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities['names'].append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities['organizations'].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            entities['dates'].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":  # Geo-Political Entity (cities, countries)\n",
    "            entities['locations'].append(ent.text)\n",
    "    \n",
    "    # Extract email using regex\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    entities['emails'] = emails\n",
    "    \n",
    "    # Extract phone numbers using regex\n",
    "    phone_pattern = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "    entities['phones'] = phones\n",
    "    \n",
    "    # Extract skills (simple keyword matching)\n",
    "    skill_keywords = ['python', 'java', 'sql', 'machine learning', 'data science', \n",
    "                     'tensorflow', 'pandas', 'numpy', 'git', 'docker', 'aws',\n",
    "                     'javascript', 'react', 'node', 'mongodb', 'tableau',\n",
    "                     'power bi', 'excel', 'r programming', 'spark', 'hadoop',\n",
    "                     'nlp', 'deep learning', 'scikit-learn', 'flask', 'django']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for skill in skill_keywords:\n",
    "        if skill in text_lower:\n",
    "            entities['skills'].append(skill)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for key in entities:\n",
    "        entities[key] = list(set(entities[key]))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test on one resume\n",
    "print(\"üîç EXTRACTING ENTITIES FROM PRIYA SHARMA'S RESUME:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "extracted = extract_entities_from_resume(sample_resume)\n",
    "\n",
    "for category, items in extracted.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚úì {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Entity extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83652308-0f90-4c3a-aaad-7da8b573661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ PARSING ALL RESUMES...\n",
      "======================================================================\n",
      "\n",
      "üìÑ Processing: resume_anjali_patel.txt\n",
      "   ‚úÖ Extracted 1 emails, 14 skills, 10 organizations\n",
      "\n",
      "üìÑ Processing: resume_priya_sharma.txt\n",
      "   ‚úÖ Extracted 1 emails, 11 skills, 9 organizations\n",
      "\n",
      "üìÑ Processing: resume_rahul_verma.txt\n",
      "   ‚úÖ Extracted 1 emails, 1 skills, 10 organizations\n",
      "\n",
      "======================================================================\n",
      "üéâ Successfully parsed 3 resumes!\n"
     ]
    }
   ],
   "source": [
    "# Parse all resumes\n",
    "parsed_resumes = {}\n",
    "\n",
    "print(\"üîÑ PARSING ALL RESUMES...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for filename, content in all_resumes.items():\n",
    "    print(f\"\\nüìÑ Processing: {filename}\")\n",
    "    entities = extract_entities_from_resume(content)\n",
    "    parsed_resumes[filename] = entities\n",
    "    print(f\"   ‚úÖ Extracted {len(entities['emails'])} emails, {len(entities['skills'])} skills, {len(entities['organizations'])} organizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéâ Successfully parsed {len(parsed_resumes)} resumes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00789d59-2f6b-4445-ad4b-45cffaa87e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved parsed resumes to: ../outputs/parsed_resumes.json\n",
      "\n",
      "üìã SAMPLE OUTPUT (Priya Sharma's Resume):\n",
      "======================================================================\n",
      "{\n",
      "  \"names\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Pandas\",\n",
      "    \"Java\",\n",
      "    \"Jupyter\\n- Soft\",\n",
      "    \"Docker\"\n",
      "  ],\n",
      "  \"organizations\": [\n",
      "    \"Data Scientist\",\n",
      "    \"CERTIFICATIONS\",\n",
      "    \"Google Data Analytics Professional Certificate\",\n",
      "    \"Bachelor of Technology\",\n",
      "    \"TensorFlow\",\n",
      "    \"IIT\",\n",
      "    \"GPA\",\n",
      "    \"Python\",\n",
      "    \"Data Analysis\"\n",
      "  ],\n",
      "  \"dates\": [\n",
      "    \"June 2021 - Present\",\n",
      "    \"Jan 2020 -\",\n",
      "    \"10+\",\n",
      "    \"2021\",\n",
      "    \"3 years\"\n",
      "  ],\n",
      "  \"locations\": [\n",
      "    \"SHARMA\",\n",
      "    \"Flask\",\n",
      "    \"India\",\n",
      "    \"Delhi\"\n",
      "  ],\n",
      "  \"emails\": [\n",
      "    \"priya.sharma@email.com\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"+91-9876543210\"\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"scikit-learn\",\n",
      "    \"docker\",\n",
      "    \"java\",\n",
      "    \"aws\",\n",
      "    \"machine learning\",\n",
      "    \"python\",\n",
      "    \"tensorflow\",\n",
      "    \"sql\",\n",
      "    \"git\",\n",
      "    \"pandas\",\n",
      "    \"flask\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save parsed data to JSON\n",
    "output_file = \"../outputs/parsed_resumes.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_resumes, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Saved parsed resumes to: {output_file}\")\n",
    "\n",
    "# Display one example\n",
    "print(\"\\nüìã SAMPLE OUTPUT (Priya Sharma's Resume):\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(parsed_resumes['resume_priya_sharma.txt'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "856b291a-13bc-4d33-9c24-5eb6ddf91655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PARSED RESUME SUMMARY:\n",
      "======================================================================\n",
      "               Filename                  Name                      Email          Phone                                            Skills                                                                  Organizations  Total Skills\n",
      "resume_anjali_patel.txt            Matplotlib anjali.patel2024@email.com +91-9988776655 nlp, scikit-learn, java, machine learning, python                          JavaScript, INTERNSHIPS\\n\\nData Analytics Intern, SQL            14\n",
      "resume_priya_sharma.txt      Machine Learning     priya.sharma@email.com +91-9876543210 scikit-learn, docker, java, aws, machine learning Data Scientist, CERTIFICATIONS, Google Data Analytics Professional Certificate            11\n",
      " resume_rahul_verma.txt Jamshedpur\\nCompleted      rahul.verma@gmail.com +91-9123456789                                               git                             MediaHub Agency, Marketing Today, Content Strategy             1\n",
      "\n",
      "üíæ Saved to CSV: ../outputs/parsed_resumes.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert to structured DataFrame\n",
    "structured_data = []\n",
    "\n",
    "for filename, entities in parsed_resumes.items():\n",
    "    structured_data.append({\n",
    "        'Filename': filename,\n",
    "        'Name': entities['names'][0] if entities['names'] else 'Unknown',\n",
    "        'Email': entities['emails'][0] if entities['emails'] else 'N/A',\n",
    "        'Phone': entities['phones'][0] if entities['phones'] else 'N/A',\n",
    "        'Skills': ', '.join(entities['skills'][:5]),  # First 5 skills\n",
    "        'Organizations': ', '.join(entities['organizations'][:3]),  # First 3 orgs\n",
    "        'Total Skills': len(entities['skills'])\n",
    "    })\n",
    "\n",
    "df_parsed = pd.DataFrame(structured_data)\n",
    "\n",
    "print(\"üìä PARSED RESUME SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_parsed.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_output = \"../outputs/parsed_resumes.csv\"\n",
    "df_parsed.to_csv(csv_output, index=False)\n",
    "print(f\"\\nüíæ Saved to CSV: {csv_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8214420d-4711-472d-9e3f-990131f4530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Opening folder in File Explorer...\n",
      "üìç Location: C:\\Users\\GURU IS GREAT\\Documents\\Resume_Parser_Project\\outputs\n",
      "\n",
      "Your files are:\n",
      "  üìÑ parsed_resumes.json\n",
      "  üìÑ parsed_resumes.csv\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Open the outputs folder\n",
    "output_path = r\"C:\\Users\\GURU IS GREAT\\Documents\\Resume_Parser_Project\\outputs\"\n",
    "subprocess.Popen(f'explorer \"{output_path}\"')\n",
    "print(f\"üóÇÔ∏è Opening folder in File Explorer...\")\n",
    "print(f\"üìç Location: {output_path}\")\n",
    "print(\"\\nYour files are:\")\n",
    "print(\"  üìÑ parsed_resumes.json\")\n",
    "print(\"  üìÑ parsed_resumes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09dccd70-515d-4255-b5fd-5f9aa003624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ PARSING JOB DESCRIPTIONS...\n",
      "======================================================================\n",
      "\n",
      "üìÑ Processing: job_data_scientist.txt\n",
      "   ‚úÖ Found 13 required skills\n",
      "   üìç Location: India\n",
      "\n",
      "üìÑ Processing: job_marketing_manager.txt\n",
      "   ‚úÖ Found 7 required skills\n",
      "   üìç Location: Mumbai\n",
      "\n",
      "======================================================================\n",
      "üéâ Successfully parsed 2 job descriptions!\n",
      "\n",
      "üìã SAMPLE JOB PARSING:\n",
      "{\n",
      "  \"job_title\": \"Data Scientist\",\n",
      "  \"company\": \"\",\n",
      "  \"location\": \"India\",\n",
      "  \"required_skills\": [\n",
      "    \"nlp\",\n",
      "    \"scikit-learn\",\n",
      "    \"aws\",\n",
      "    \"machine learning\",\n",
      "    \"python\",\n",
      "    \"tensorflow\",\n",
      "    \"data science\",\n",
      "    \"spark\",\n",
      "    \"power bi\",\n",
      "    \"sql\",\n",
      "    \"analytics\",\n",
      "    \"tableau\",\n",
      "    \"hadoop\"\n",
      "  ],\n",
      "  \"experience_required\": [],\n",
      "  \"education_required\": [],\n",
      "  \"organizations_mentioned\": [\n",
      "    \"NLP\",\n",
      "    \"GCP\",\n",
      "    \"TensorFlow\",\n",
      "    \"Computer Science, Statistics\",\n",
      "    \"PyTorch\",\n",
      "    \"TechVentures Pvt\",\n",
      "    \"Data Scientist\",\n",
      "    \"Tableau, Power BI\",\n",
      "    \"Spark, Hadoop\",\n",
      "    \"AI\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_job_requirements(text):\n",
    "    \"\"\"\n",
    "    Extract requirements from job description\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    job_info = {\n",
    "        'job_title': '',\n",
    "        'company': '',\n",
    "        'location': '',\n",
    "        'required_skills': [],\n",
    "        'experience_required': [],\n",
    "        'education_required': [],\n",
    "        'organizations_mentioned': []\n",
    "    }\n",
    "    \n",
    "    # Extract entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            job_info['organizations_mentioned'].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            if not job_info['location']:\n",
    "                job_info['location'] = ent.text\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            job_info['experience_required'].append(ent.text)\n",
    "    \n",
    "    # Extract skills\n",
    "    skill_keywords = ['python', 'java', 'sql', 'machine learning', 'data science', \n",
    "                     'tensorflow', 'pandas', 'numpy', 'git', 'docker', 'aws',\n",
    "                     'javascript', 'react', 'node', 'mongodb', 'tableau',\n",
    "                     'power bi', 'excel', 'r programming', 'spark', 'hadoop',\n",
    "                     'nlp', 'deep learning', 'scikit-learn', 'flask', 'django',\n",
    "                     'seo', 'sem', 'digital marketing', 'analytics', 'marketing']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for skill in skill_keywords:\n",
    "        if skill in text_lower:\n",
    "            job_info['required_skills'].append(skill)\n",
    "    \n",
    "    # Extract job title (usually first line or after \"TITLE:\")\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines[:5]:\n",
    "        if 'title:' in line.lower() or 'position:' in line.lower():\n",
    "            job_info['job_title'] = line.split(':')[-1].strip()\n",
    "            break\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for key in ['required_skills', 'organizations_mentioned']:\n",
    "        job_info[key] = list(set(job_info[key]))\n",
    "    \n",
    "    return job_info\n",
    "\n",
    "# Parse all job descriptions\n",
    "parsed_jobs = {}\n",
    "\n",
    "print(\"üîÑ PARSING JOB DESCRIPTIONS...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for filename, content in all_jobs.items():\n",
    "    print(f\"\\nüìÑ Processing: {filename}\")\n",
    "    job_data = extract_job_requirements(content)\n",
    "    parsed_jobs[filename] = job_data\n",
    "    print(f\"   ‚úÖ Found {len(job_data['required_skills'])} required skills\")\n",
    "    print(f\"   üìç Location: {job_data['location']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéâ Successfully parsed {len(parsed_jobs)} job descriptions!\")\n",
    "\n",
    "# Display one example\n",
    "print(\"\\nüìã SAMPLE JOB PARSING:\")\n",
    "print(json.dumps(parsed_jobs['job_data_scientist.txt'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f198b6c6-80f6-407a-947f-a716e411b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MATCHING RESUMES TO JOBS...\n",
      "======================================================================\n",
      "            Candidate             Resume File                 Job Title                  Job File  Match Score (%)                                                                    Matched Skills  Total Matched Skills\n",
      "           Matplotlib resume_anjali_patel.txt            Data Scientist    job_data_scientist.txt            61.54 nlp, scikit-learn, machine learning, python, data science, power bi, sql, tableau                     8\n",
      "     Machine Learning resume_priya_sharma.txt            Data Scientist    job_data_scientist.txt            46.15                      scikit-learn, aws, machine learning, python, tensorflow, sql                     6\n",
      "           Matplotlib resume_anjali_patel.txt Digital Marketing Manager job_marketing_manager.txt            14.29                                                                               git                     1\n",
      "     Machine Learning resume_priya_sharma.txt Digital Marketing Manager job_marketing_manager.txt            14.29                                                                               git                     1\n",
      "Jamshedpur\\nCompleted  resume_rahul_verma.txt Digital Marketing Manager job_marketing_manager.txt            14.29                                                                               git                     1\n",
      "Jamshedpur\\nCompleted  resume_rahul_verma.txt            Data Scientist    job_data_scientist.txt             0.00                                                                                                       0\n",
      "\n",
      "======================================================================\n",
      "üíæ Saved matching results to: resume_job_matches.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_match_score(resume_entities, job_requirements):\n",
    "    \"\"\"\n",
    "    Calculate how well a resume matches a job description\n",
    "    \"\"\"\n",
    "    resume_skills = set(resume_entities['skills'])\n",
    "    job_skills = set(job_requirements['required_skills'])\n",
    "    \n",
    "    if not job_skills:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate skill match percentage\n",
    "    matching_skills = resume_skills.intersection(job_skills)\n",
    "    match_score = (len(matching_skills) / len(job_skills)) * 100\n",
    "    \n",
    "    return round(match_score, 2), list(matching_skills)\n",
    "\n",
    "# Match all resumes to all jobs\n",
    "print(\"üéØ MATCHING RESUMES TO JOBS...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "matching_results = []\n",
    "\n",
    "for resume_file, resume_data in parsed_resumes.items():\n",
    "    candidate_name = resume_data['names'][0] if resume_data['names'] else 'Unknown'\n",
    "    \n",
    "    for job_file, job_data in parsed_jobs.items():\n",
    "        job_title = job_data['job_title'] or job_file.replace('.txt', '')\n",
    "        \n",
    "        score, matched_skills = calculate_match_score(resume_data, job_data)\n",
    "        \n",
    "        matching_results.append({\n",
    "            'Candidate': candidate_name,\n",
    "            'Resume File': resume_file,\n",
    "            'Job Title': job_title,\n",
    "            'Job File': job_file,\n",
    "            'Match Score (%)': score,\n",
    "            'Matched Skills': ', '.join(matched_skills),\n",
    "            'Total Matched Skills': len(matched_skills)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_matches = pd.DataFrame(matching_results)\n",
    "df_matches = df_matches.sort_values('Match Score (%)', ascending=False)\n",
    "\n",
    "print(df_matches.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Save matching results\n",
    "match_csv = \"resume_job_matches.csv\"\n",
    "df_matches.to_csv(match_csv, index=False)\n",
    "print(f\"üíæ Saved matching results to: {match_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b7c1ac-d806-43a3-b501-47af015d745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PARSING STATISTICS:\n",
      "======================================================================\n",
      "\n",
      "üìÑ RESUMES PROCESSED: 3\n",
      "\n",
      "  resume_anjali_patel.txt:\n",
      "    Emails found: 1\n",
      "    Skills found: 14\n",
      "    Organizations: 10\n",
      "    Dates extracted: 5\n",
      "\n",
      "  resume_priya_sharma.txt:\n",
      "    Emails found: 1\n",
      "    Skills found: 11\n",
      "    Organizations: 9\n",
      "    Dates extracted: 5\n",
      "\n",
      "  resume_rahul_verma.txt:\n",
      "    Emails found: 1\n",
      "    Skills found: 1\n",
      "    Organizations: 10\n",
      "    Dates extracted: 7\n",
      "\n",
      "\n",
      "üíº JOB DESCRIPTIONS PROCESSED: 2\n",
      "\n",
      "  job_data_scientist.txt:\n",
      "    Required skills: 13\n",
      "    Location: India\n",
      "\n",
      "  job_marketing_manager.txt:\n",
      "    Required skills: 7\n",
      "    Location: Mumbai\n",
      "\n",
      "\n",
      "üéØ MATCHING RESULTS:\n",
      "======================================================================\n",
      "Total matches calculated: 6\n",
      "Best match score: 61.54%\n",
      "Average match score: 25.09%\n",
      "\n",
      "üèÜ TOP 3 MATCHES:\n",
      "       Candidate                 Job Title  Match Score (%)\n",
      "      Matplotlib            Data Scientist            61.54\n",
      "Machine Learning            Data Scientist            46.15\n",
      "      Matplotlib Digital Marketing Manager            14.29\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall statistics\n",
    "print(\"üìä PARSING STATISTICS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÑ RESUMES PROCESSED: {len(parsed_resumes)}\")\n",
    "for filename, data in parsed_resumes.items():\n",
    "    print(f\"\\n  {filename}:\")\n",
    "    print(f\"    Emails found: {len(data['emails'])}\")\n",
    "    print(f\"    Skills found: {len(data['skills'])}\")\n",
    "    print(f\"    Organizations: {len(data['organizations'])}\")\n",
    "    print(f\"    Dates extracted: {len(data['dates'])}\")\n",
    "\n",
    "print(f\"\\n\\nüíº JOB DESCRIPTIONS PROCESSED: {len(parsed_jobs)}\")\n",
    "for filename, data in parsed_jobs.items():\n",
    "    print(f\"\\n  {filename}:\")\n",
    "    print(f\"    Required skills: {len(data['required_skills'])}\")\n",
    "    print(f\"    Location: {data['location']}\")\n",
    "\n",
    "print(\"\\n\\nüéØ MATCHING RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total matches calculated: {len(df_matches)}\")\n",
    "print(f\"Best match score: {df_matches['Match Score (%)'].max()}%\")\n",
    "print(f\"Average match score: {df_matches['Match Score (%)'].mean():.2f}%\")\n",
    "\n",
    "print(\"\\nüèÜ TOP 3 MATCHES:\")\n",
    "print(df_matches.head(3)[['Candidate', 'Job Title', 'Match Score (%)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11b4da24-3bb4-4ce7-bcaa-55ed7f25e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All files saved to outputs folder!\n",
      "\n",
      "Files created:\n",
      "  üìÑ parsed_resumes.json\n",
      "  üìÑ parsed_jobs.json\n",
      "  üìÑ resume_job_matches.csv\n",
      "  üìÑ parsed_resumes_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: 'explorer \"C:\\\\Users\\\\GURU IS GREAT\\\\Document...>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save all outputs to the outputs folder\n",
    "output_folder = r\"C:\\Users\\GURU IS GREAT\\Documents\\Resume_Parser_Project\\outputs\"\n",
    "\n",
    "# Save parsed resumes\n",
    "with open(os.path.join(output_folder, \"parsed_resumes.json\"), 'w') as f:\n",
    "    json.dump(parsed_resumes, f, indent=4)\n",
    "\n",
    "# Save parsed jobs\n",
    "with open(os.path.join(output_folder, \"parsed_jobs.json\"), 'w') as f:\n",
    "    json.dump(parsed_jobs, f, indent=4)\n",
    "\n",
    "# Save matches\n",
    "df_matches.to_csv(os.path.join(output_folder, \"resume_job_matches.csv\"), index=False)\n",
    "\n",
    "# Save resume summary\n",
    "df_parsed.to_csv(os.path.join(output_folder, \"parsed_resumes_summary.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ All files saved to outputs folder!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  üìÑ parsed_resumes.json\")\n",
    "print(\"  üìÑ parsed_jobs.json\")\n",
    "print(\"  üìÑ resume_job_matches.csv\")\n",
    "print(\"  üìÑ parsed_resumes_summary.csv\")\n",
    "\n",
    "# Open folder\n",
    "subprocess.Popen(f'explorer \"{output_folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8fa78-14f9-46d2-92d7-d85c007cbc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
